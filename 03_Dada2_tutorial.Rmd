---
title: "Dada2 tutorial"
output: 
  github_document:
    toc: true
    toc_depth: 2
---
# Préparation de l'environnement
## Chargement des librairies Phyloseq et Dada2 afin de pouvoir utiliser ses packages
```{r}
library ("phyloseq")
library("dada2")
```

## Création d'une variable 'path' dans laquelle on met l'objet "Miseq_SOP". Ensuite on les liste
```{r}
path <- "~/MiSeq_SOP" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```
## Création de deux listes. 
Une avec les Forwards et une avec les Reverses qui correspondent respectivement au Read1 et Read2. La commande avec 'sample.names' permet de mettre toutes les séquences sous un même format pour harmoniser leurs noms.
```{r}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

# Inspect read quality profiles
Permet d'inspecter la qualité des différents reads et nous permet de voir à partir de quel nucléotide nous allons avoir une baisse du score de qualité pour pouvoir par la suite retirer les nucléotides de mauvaise qualité. Sur les forwards nous allons retirer les 10 derniers nucléotides avec une commande retrouvée plus bas dans le script. 
```{r}
plotQualityProfile(fnRs[1:4])
```
Ici nous faisons la même manipulation mais avec les Reverses qui sont d'un peu moins bonne qualité que les Forwards, dû au sequençage avec Illumina.Ici, nous retirerons tous les nucléotides à partir du 160e
```{r}
plotQualityProfile(fnRs[1:4])
```
# Filter and trim
Ici nous créeons une nouvelle variable qui recevra tous les fichiers filtrés, que ce soit pour les Forwards ou les Reverses. Nous allons en même temps indiquer à la machine que les noms utilisés pour ranger les séquences dans les dossiers seront les mêmes que nous avons standardisé plus haut.
```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Nous allons indiquer à la machine quels paramètres nous allons utiliser pour filtrer les séquences avant de les ranger. On indique ainsi dans la première fonction les deux fichiers d'où les séquences viendront (fnFs et fnRs), puis le fichier où elles seront rangées (filtFs et filtRs crées juste au dessus). Enuite nous allons indiquer où couper pour les deux sortes de séquences. 
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```
# Learn the Error Rates
 Nous allons ici utiliser des lignes de commandes qui vont permettre d'apprendre à la machine les différents profils d'erreurs générées lors du séquençage. L'opération est faite sur les deux types de séquence.
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
```

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```
Cette ligne nous permet de visualiser les erreurs qu'on a fait apprendre à la machine
```{r}
plotErrors(errF, nominalQ=TRUE)
```

# Sample Inference
Ici nous créons une autre variable "dadaFs" dans laquelle nous mettons les fichiers obtenus après avoir filtré et appliqué le profil d'erreur à nos séquences. Nous allons faire la même chose avec dadaRS.
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```
```{r}
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```
Cette commande nous permet de visualiser le résultat global qu'on retrouve classé dans la liste dadaFs. Ils nous indiquent que sur les séquences on retrouve 128 séquences qui correspondent aux vrais variants, par rapport aux 1979 séquences. Ils nous indiquent aussi les diagnostiques de qualité.
```{r}
dadaFs[[1]]
```
# Merge paired reads
Ici nous voulons mettre en une seule séquence les Forwards et les Reverses.Nous pouvons faire cette opération grâce aux overlaps de 12 paires de base. Cela se fait grâce à un alignement entre les forwards et les reverses qui vont permettre de contruire les contigs.
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```
# Construct sequence table
Nous allons construire une table des variations de séquence dans les amplicons (ASV) qui permet une meilleure résolution que les tables OTUs 97%
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```
```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

# Remove chimeras
Malgré qu'on ait pu appliquer les modèles d'erreurs aux séquences, il reste des chimères. Ces chimères sont facilement reconnaissables par la machine et peuvent etre réparées en y rajoutant les parties droites et gauche des 2 séquences les plus abondantes.
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
Ici on peut voir qu'on à 3% de séquences chimériques dans notre jeu de donnée.
```{r}
1-sum(seqtab.nochim)/sum(seqtab)
```


# Track reads through the pipeline
Ce code nous permet de visualiser le nombre de séquences obtenues à la suite de toutes nos manipulations de filtrage. Ici nous pouvons voir qu'on a pu récupérer la plupart de nos séquences brutes, ce qui est signe d'une bonne qualité de séquençage.
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
# Assign taxonomy
Ici nous avons du récupérer silva afin d'analyser et d'assigner les taxonomies.
```{bash}
wget https://zenodo.org/record/3986799/files/silva_species_assignment_v138.fa.gz
```
Ici nous créons une variable qui va recevoir les espèces obtenues grâce à Silva

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```

```{r}
taxa <- addSpecies(taxa, "~/silva_species_assignment_v138.fa.gz")
```
On remarque donc après avoir affiché la table qu'on a créée on obtient une majorité de  Bacteroidetes ce qui est normal dans des échantillons fécaux. D'autres espèces n'ont pas pu être assignées car on a peu de données sur les bactéries des intestins des souris. 
```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

# Evaluate accuracy
Ici on cherche à comparer les variants donnés par la machine par rapport à la composition de communauté attendue. Le premier code nous montre qu'on s'attendait à avoir 20 souches différentes.
```{r}
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
```
Dada2 a indentifié 20 ASVs. Cela correspond précisement à ce à quoi on s'attendait, donc le taux d'erreur résiduel par dada2 est de 0%
```{r}
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
```
# Sauvegarde de l'environnement pour pouvoir jouer le code de phyloseq
```{r}
save.image(file="03_DADA2_tutorial_FinalEnv")
```







